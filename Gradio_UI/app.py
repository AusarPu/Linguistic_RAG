import os
import gradio as gr
import asyncio
import logging
import json
from typing import List, Dict, Any, Optional, Tuple
import traceback
import sys

# è®¾ç½®æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('/home/pushihao/RAG/Gradio_UI/app.log')
    ]
)

# ä»é¡¹ç›®ä¸­å¯¼å…¥
sys.path.append('/home/pushihao/RAG')
from script.knowledge_base import KnowledgeBase
from script.rag_pipeline import execute_rag_flow
from script import config_rag as config

logger = logging.getLogger(__name__)
kb_instance: Optional[KnowledgeBase] = None

# å…¨å±€çŸ¥è¯†åº“å®ä¾‹

def load_resources():
    """åŠ è½½åº”ç”¨æ‰€éœ€çš„å…¨å±€èµ„æºï¼Œä¾‹å¦‚çŸ¥è¯†åº“ã€‚"""
    global kb_instance
    logger.info("å¼€å§‹åŠ è½½çŸ¥è¯†åº“å®ä¾‹...")
    if kb_instance is None:
        try:
            kb_instance = KnowledgeBase()
            logger.info("çŸ¥è¯†åº“å®ä¾‹åŠ è½½æˆåŠŸã€‚")
        except Exception as e:
            logger.error(f"çŸ¥è¯†åº“å®ä¾‹åŒ–å¤±è´¥: {e}", exc_info=True)
            raise RuntimeError(f"æ— æ³•åŠ è½½KnowledgeBase: {e}") from e
    else:
        logger.info("çŸ¥è¯†åº“å®ä¾‹å·²å­˜åœ¨ï¼Œè·³è¿‡åŠ è½½ã€‚")

def format_markdown_collapsible(title: str, content: str, is_open: bool = False) -> str:
    """æ ¼å¼åŒ–ä¸ºå¯æŠ˜å çš„markdownå†…å®¹"""
    if not content.strip():
        return ""
    open_attr = " open" if is_open else ""
    return f"<details{open_attr}>\n<summary><strong>{title}</strong></summary>\n\n{content}\n\n</details>\n\n"

def format_stage_info(stage: str, message: str) -> str:
    """æ ¼å¼åŒ–é˜¶æ®µä¿¡æ¯"""
    stage_map = {
        "pipeline_start": "ğŸš€ æµç¨‹å¯åŠ¨",
        "query_rewriting": "âœï¸ æŸ¥è¯¢é‡æ„", 
        "retrieval_start": "ğŸ” çŸ¥è¯†æ£€ç´¢",
        "usefulness_judging": "ğŸ¯ ç›¸å…³æ€§åˆ¤æ–­",
        "generation_start": "ğŸ’­ ç­”æ¡ˆç”Ÿæˆ",
        "generation_complete": "âœ… ç”Ÿæˆå®Œæˆ"
    }
    icon_stage = stage_map.get(stage, f"ğŸ“‹ {stage}")
    return f"**{icon_stage}**: {message}"

def format_retrieved_chunks(chunks: List[Dict]) -> str:
    """æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„çŸ¥è¯†å— - å®Œæ•´æ˜¾ç¤ºå†…å®¹"""
    if not chunks:
        return "æš‚æ— æ£€ç´¢ç»“æœ"
    
    formatted_chunks = []
    for i, chunk in enumerate(chunks, 1):
        # å®Œæ•´æ˜¾ç¤ºçŸ¥è¯†å—å†…å®¹ï¼Œä¸æˆªæ–­
        chunk_content = chunk.get('text', '')
        chunk_info = f"""**ç‰‡æ®µ {i}**
- **æ–‡æ¡£**: {chunk.get('doc_name', 'æœªçŸ¥')}
- **ä½œè€…**: {chunk.get('author', 'æœªçŸ¥')}
- **é¡µç **: {chunk.get('page_number', 'æœªçŸ¥')}
- **æ¥æºè·¯å¾„**: {', '.join(chunk.get('from_paths', []))}
- **å—ID**: {chunk.get('chunk_id', 'æœªçŸ¥')}

**å®Œæ•´å†…å®¹**:
```
{chunk_content}
```

---
"""
        formatted_chunks.append(chunk_info)
    
    return "\n".join(formatted_chunks)

def format_chat_message_with_thinking(thinking_content: str, response_content: str) -> str:
    """æ ¼å¼åŒ–èŠå¤©æ¶ˆæ¯ï¼ŒåŒ…å«æ€è€ƒè¿‡ç¨‹å’Œå›ç­”å†…å®¹"""
    result = ""
    
    # æ·»åŠ æ€è€ƒå†…å®¹ï¼ˆå¯æŠ˜å ï¼‰
    if thinking_content.strip():
        result += format_markdown_collapsible("ğŸ’­ æ€è€ƒè¿‡ç¨‹", thinking_content, is_open=False)
    
    # æ·»åŠ å›ç­”å†…å®¹ï¼ˆæ­£å¸¸æ˜¾ç¤ºï¼‰
    if response_content.strip():
        result += f"**å›ç­”å†…å®¹ï¼š**\n\n{response_content}"
    
    return result

async def chat_with_rag(
    user_input: str,
    chat_history: List[Dict[str, str]],
    openai_history: List[Dict[str, str]]
):
    """RAGèŠå¤©å¤„ç†å‡½æ•°"""
    
    logger.info(f"å¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥: {user_input[:50]}...")
    
    if not user_input.strip():
        logger.warning("ç”¨æˆ·è¾“å…¥ä¸ºç©ºï¼Œè¿”å›å½“å‰çŠ¶æ€")
        yield chat_history, openai_history, "", "æš‚æ— æ£€ç´¢ç»“æœ"
        return
    
    if kb_instance is None:
        logger.error("çŸ¥è¯†åº“æœªåŠ è½½")
        error_msg = "é”™è¯¯ï¼šçŸ¥è¯†åº“æœªæˆåŠŸåŠ è½½ï¼Œè¯·æ£€æŸ¥åº”ç”¨æ—¥å¿—ã€‚"
        chat_history.append({"role": "user", "content": user_input})
        chat_history.append({"role": "assistant", "content": error_msg})
        yield chat_history, openai_history, "âŒ é”™è¯¯", "æš‚æ— æ£€ç´¢ç»“æœ"
        return
    
    # åˆå§‹åŒ–çŠ¶æ€å˜é‡
    current_stage = ""
    current_retrieved_chunks = []
    current_thinking_content = ""
    current_response_content = ""
    knowledge_ready = False  # æ ‡è®°çŸ¥è¯†åº“å†…å®¹æ˜¯å¦å‡†å¤‡å¥½æ˜¾ç¤º
    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    chat_history.append({"role": "user", "content": user_input})
    chat_history.append({"role": "assistant", "content": ""})
    openai_history.append({"role": "user", "content": user_input})
    
    # ç«‹å³è¿”å›åˆå§‹çŠ¶æ€ï¼Œéšè—çŸ¥è¯†åº“å†…å®¹
    yield chat_history, openai_history, "ğŸ” å¼€å§‹å¤„ç†...", "ğŸ” æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“..."
    
    try:
        # å¼€å§‹æ‰§è¡ŒRAGæµç¨‹
        event_count = 0
        async for event in execute_rag_flow(
            user_query=user_input,
            chat_history_openai=openai_history[:-1],
            kb_instance=kb_instance
        ):
            event_count += 1
            event_type = event.get("type")
            logger.debug(f"æ”¶åˆ°äº‹ä»¶: {event_type}")
            
            # å¤„ç†çŠ¶æ€æ›´æ–°
            if event_type == "status":
                stage = event.get("stage", "")
                message = event.get("message", "")
                current_stage = format_stage_info(stage, message)
                logger.info(f"çŠ¶æ€æ›´æ–°: {current_stage}")
                
                # åªæœ‰çŸ¥è¯†åº“å‡†å¤‡å¥½åæ‰æ˜¾ç¤ºå†…å®¹ï¼Œå¦åˆ™æ˜¾ç¤ºç­‰å¾…ä¿¡æ¯
                knowledge_display_content = format_retrieved_chunks(current_retrieved_chunks) if knowledge_ready else "ğŸ” æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“..."
                
                # æ›´æ–°èŠå¤©å†å²ä¸­çš„åŠ©æ‰‹å›å¤
                if chat_history and chat_history[-1].get("role") == "assistant":
                    chat_history[-1]["content"] = format_chat_message_with_thinking(current_thinking_content, current_response_content)
                
                yield chat_history, openai_history, current_stage, knowledge_display_content
            
            # å¤„ç†æ£€ç´¢ç»“æœ
            elif event_type == "useful_chunks_preview":
                chunks = event.get("preview", [])
                current_retrieved_chunks = chunks
                knowledge_ready = True  # æ ‡è®°çŸ¥è¯†åº“å†…å®¹å‡†å¤‡å¥½
                logger.info(f"æ£€ç´¢ç»“æœæ›´æ–°: è·å¾— {len(chunks)} ä¸ªçŸ¥è¯†å—")
                
                # æ›´æ–°èŠå¤©å†å²ä¸­çš„åŠ©æ‰‹å›å¤
                if chat_history and chat_history[-1].get("role") == "assistant":
                    chat_history[-1]["content"] = format_chat_message_with_thinking(current_thinking_content, current_response_content)
                
                yield chat_history, openai_history, current_stage, format_retrieved_chunks(current_retrieved_chunks)
            
            # å¤„ç†æ€è€ƒå†…å®¹ï¼ˆæµå¼ï¼‰
            elif event_type == "reasoning_delta":
                reasoning_text = event.get("text", "")
                current_thinking_content += reasoning_text
                # æ›´æ–°æ€è€ƒå†…å®¹
                
                # å®æ—¶æ›´æ–°èŠå¤©å†å²ä¸­çš„åŠ©æ‰‹å›å¤
                if chat_history and chat_history[-1].get("role") == "assistant":
                    chat_history[-1]["content"] = format_chat_message_with_thinking(current_thinking_content, current_response_content)
                
                knowledge_display_content = format_retrieved_chunks(current_retrieved_chunks) if knowledge_ready else "ğŸ” æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“..."
                yield chat_history, openai_history, current_stage, knowledge_display_content
            
            # å¤„ç†å›ç­”å†…å®¹ï¼ˆæµå¼ï¼‰
            elif event_type == "content_delta":
                content_text = event.get("text", "")
                current_response_content += content_text
                # æ›´æ–°å›ç­”å†…å®¹
                
                # å®æ—¶æ›´æ–°èŠå¤©å†å²ä¸­çš„åŠ©æ‰‹å›å¤
                if chat_history and chat_history[-1].get("role") == "assistant":
                    chat_history[-1]["content"] = format_chat_message_with_thinking(current_thinking_content, current_response_content)
                
                knowledge_display_content = format_retrieved_chunks(current_retrieved_chunks) if knowledge_ready else "ğŸ” æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“..."
                yield chat_history, openai_history, current_stage, knowledge_display_content
            
            # å¤„ç†æµç¨‹ç»“æŸ
            elif event_type == "pipeline_end":
                logger.info("RAGæµç¨‹ç»“æŸ")
                # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°openaiå†å²
                if current_response_content.strip():
                    openai_history.append({"role": "assistant", "content": current_response_content.strip()})
                
                # æœ€ç»ˆæ›´æ–°èŠå¤©å†å²
                if chat_history and chat_history[-1].get("role") == "assistant":
                    chat_history[-1]["content"] = format_chat_message_with_thinking(current_thinking_content, current_response_content)
                
                current_stage = "âœ… æµç¨‹å®Œæˆ"
                knowledge_display_content = format_retrieved_chunks(current_retrieved_chunks) if knowledge_ready else "æš‚æ— æ£€ç´¢ç»“æœ"
                yield chat_history, openai_history, current_stage, knowledge_display_content
                break
            
            else:
                logger.warning(f"æœªå¤„ç†çš„äº‹ä»¶ç±»å‹: {event_type}")
        
        logger.info("RAGæµç¨‹å®Œæˆ")
    
    except Exception as e:
        error_msg = f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg, exc_info=True)
        # è®°å½•é”™è¯¯å †æ ˆåˆ°æ—¥å¿—æ–‡ä»¶
        
        if chat_history and chat_history[-1].get("role") == "assistant":
            chat_history[-1]["content"] = error_msg
        
        current_stage = "âŒ é”™è¯¯"
        knowledge_display_content = format_retrieved_chunks(current_retrieved_chunks) if knowledge_ready else "æš‚æ— æ£€ç´¢ç»“æœ"
        yield chat_history, openai_history, current_stage, knowledge_display_content

def create_ui():
    """åˆ›å»ºGradioç•Œé¢"""
    with gr.Blocks(title="RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ", theme=gr.themes.Soft()) as app:
        gr.Markdown("# ğŸš€ RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
        gr.Markdown("åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
        
        # çŠ¶æ€å˜é‡
        chat_history_state = gr.State([])
        openai_history_state = gr.State([])
        
        with gr.Row():
            # å·¦æ ï¼šå¯¹è¯çª—å£ (70%)
            with gr.Column(scale=7):
                gr.Markdown("## ğŸ’¬ å¯¹è¯çª—å£")
                
                # èŠå¤©ç•Œé¢
                chatbot = gr.Chatbot(
                    label="å¯¹è¯å†å²",
                    height=500,
                    show_copy_button=True,
                    type="messages"
                )
                
                # è¾“å…¥æ¡†
                with gr.Row():
                    user_input = gr.Textbox(
                        label="è¾“å…¥æ‚¨çš„é—®é¢˜",
                        placeholder="è¯·è¾“å…¥æ‚¨æƒ³è¦è¯¢é—®çš„é—®é¢˜...",
                        scale=4
                    )
                    submit_btn = gr.Button("å‘é€", variant="primary", scale=1)
            
            # å³æ ï¼šçŠ¶æ€å’ŒçŸ¥è¯†åº“å†…å®¹ (30%)
            with gr.Column(scale=3):
                gr.Markdown("## ğŸ“Š ç³»ç»ŸçŠ¶æ€")
                
                # å½“å‰é˜¶æ®µæ˜¾ç¤º
                current_stage_display = gr.Markdown(
                    label="å½“å‰é˜¶æ®µ",
                    value="ç­‰å¾…ç”¨æˆ·è¾“å…¥..."
                )
                
                gr.Markdown("## ğŸ“š æ£€ç´¢åˆ°çš„çŸ¥è¯†")
                
                # çŸ¥è¯†åº“å†…å®¹å±•ç¤º
                knowledge_display = gr.Markdown(
                    label="çŸ¥è¯†åº“å†…å®¹",
                    value="æš‚æ— æ£€ç´¢ç»“æœ",
                    height=400
                )
        

        
        # æ§åˆ¶æŒ‰é’®
        with gr.Row():
            clear_btn = gr.Button("æ¸…ç©ºå¯¹è¯", variant="secondary")
            reload_kb_btn = gr.Button("é‡æ–°åŠ è½½çŸ¥è¯†åº“", variant="secondary")
        
        # äº‹ä»¶å¤„ç†å‡½æ•°
        def clear_chat():
            logger.info("ç”¨æˆ·ç‚¹å‡»æ¸…ç©ºå¯¹è¯")
            return [], [], "ç­‰å¾…ç”¨æˆ·è¾“å…¥...", "æš‚æ— æ£€ç´¢ç»“æœ"
        
        def reload_knowledge_base():
            global kb_instance
            logger.info("ç”¨æˆ·ç‚¹å‡»é‡æ–°åŠ è½½çŸ¥è¯†åº“")
            try:
                kb_instance = None
                load_resources()
                return "çŸ¥è¯†åº“é‡æ–°åŠ è½½æˆåŠŸ"
            except Exception as e:
                logger.error(f"é‡æ–°åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}", exc_info=True)
                return f"çŸ¥è¯†åº“é‡æ–°åŠ è½½å¤±è´¥: {str(e)}"
        
        # ç»‘å®šäº‹ä»¶
        submit_btn.click(
            fn=chat_with_rag,
            inputs=[user_input, chat_history_state, openai_history_state],
            outputs=[chatbot, openai_history_state, current_stage_display, knowledge_display]
        ).then(
            fn=lambda: "",
            inputs=[],
            outputs=[user_input]
        )
        
        user_input.submit(
            fn=chat_with_rag,
            inputs=[user_input, chat_history_state, openai_history_state],
            outputs=[chatbot, openai_history_state, current_stage_display, knowledge_display]
        ).then(
            fn=lambda: "",
            inputs=[],
            outputs=[user_input]
        )
        
        clear_btn.click(
            fn=clear_chat,
            outputs=[chat_history_state, openai_history_state, current_stage_display, knowledge_display]
        ).then(
            fn=lambda: [],
            outputs=[chatbot]
        )
        
        reload_kb_btn.click(
            fn=reload_knowledge_base,
            outputs=[]
        )
    
    return app

if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—
    config.setup_logging()
    
    try:
        logger.info("å¼€å§‹å¯åŠ¨RAG WebUI")
        load_resources()
        logger.info("åº”ç”¨èµ„æºåŠ è½½å®Œæˆã€‚")
        
        app = create_ui()
        logger.info("Gradioåº”ç”¨æ„å»ºå®Œæˆï¼Œå‡†å¤‡å¯åŠ¨...")
        app.queue().launch(
            server_name="0.0.0.0", 
            server_port=8080, 
            share=False,
            show_error=True,
            debug=False
        )
        logger.info("Gradioåº”ç”¨å·²å¯åŠ¨åœ¨ç«¯å£ 8080ã€‚")
    
    except RuntimeError as e:
        logger.critical(f"åº”ç”¨å¯åŠ¨å¤±è´¥: {e}", exc_info=True)
    except Exception as e:
        logger.critical(f"å‘ç”ŸæœªçŸ¥é”™è¯¯å¯¼è‡´åº”ç”¨æ— æ³•å¯åŠ¨: {e}", exc_info=True)