# app_gradio.py (ç¬¬ä¸€éƒ¨åˆ†ï¼šå¯¼å…¥ã€å…¨å±€å˜é‡ã€è¾…åŠ©å‡½æ•°)

import gradio as gr
import time
import logging
from pathlib import Path
from typing import List, Tuple, Dict, Optional, AsyncGenerator, Union, Any
import json
import asyncio
import os  # ç”¨äºè·¯å¾„å¤„ç†ï¼Œç¡®ä¿å¯¼å…¥
import sys  # ç”¨äºè·¯å¾„å¤„ç†ï¼Œç¡®ä¿å¯¼å…¥

try:
    import aiohttp
except ImportError:
    print("Please install aiohttp: pip install aiohttp")
    exit(1)

# è·å–å½“å‰æ–‡ä»¶çš„ç›®å½•ï¼Œç„¶åè·å–é¡¹ç›®æ ¹ç›®å½•
_CURRENT_FILE_DIR = Path(__file__).resolve().parent
PROJECT_ROOT_DIR_FOR_APP = _CURRENT_FILE_DIR  # å‡è®¾ app_gradio.py åœ¨é¡¹ç›®æ ¹ç›®å½•

_SCRIPT_DIR_PATH = PROJECT_ROOT_DIR_FOR_APP / "script"
_WEB_UI_DIR_PATH = PROJECT_ROOT_DIR_FOR_APP / "web_ui"
if str(_SCRIPT_DIR_PATH) not in sys.path:
    sys.path.insert(0, str(_SCRIPT_DIR_PATH))
if str(PROJECT_ROOT_DIR_FOR_APP) not in sys.path:  # ç¡®ä¿æ ¹ç›®å½•ä¹Ÿåœ¨
    sys.path.insert(0, str(PROJECT_ROOT_DIR_FOR_APP))

try:
    # åç«¯é€»è¾‘å¯¼å…¥
    from script.knowledge_base import KnowledgeBase
    from script import config  # å¯¼å…¥ config æ¨¡å—æœ¬èº«
    # from script.config import * # å¯¼å…¥æ‰€æœ‰é…ç½®å˜é‡ (æŒ‰éœ€é€‰æ‹©)
    # æˆ‘ä»¬ä¼šåœ¨éœ€è¦çš„åœ°æ–¹ç”¨ config.VARIABLE_NAME

    # æ ¸å¿ƒRAGæµç¨‹
    from script.rag_pipeline import execute_rag_flow  # <--- æ–°å¢å¯¼å…¥

    # UI å¸ƒå±€å¯¼å…¥
    from web_ui.layout import create_layout  # å‡è®¾å®ƒåœ¨ web_ui/layout.py

except ImportError as e:
    # å°è¯•å¦ä¸€ç§å¯¼å…¥æ–¹å¼ï¼Œå¦‚æœ app_gradio.py å°±åœ¨ script ç›®å½•å¹³çº§
    # æˆ–è€…ä½ ç›´æ¥ä» script/ è¿è¡Œæ­¤æ–‡ä»¶
    try:
        from knowledge_base import KnowledgeBase
        import config
        from rag_pipeline import execute_rag_flow
        from web_ui.layout import create_layout  # web_ui ç›®å½•éœ€è¦èƒ½è¢«æ‰¾åˆ°
    except ImportError as e_inner:
        print(f"Error importing modules: {e} / {e_inner}")
        print("Please ensure all necessary files (config.py, knowledge_base.py, rag_pipeline.py etc.) "
              "exist and PYTHONPATH is correct, or adjust import paths.")
        print(f"Current sys.path: {sys.path}")
        print(f"Attempted SCRIPT_DIR: {_SCRIPT_DIR_PATH}, WEB_UI_DIR: {_WEB_UI_DIR_PATH}")
        exit(999)

# --- Logging Setup (ä¸ä½ ç°æœ‰é€»è¾‘ä¿æŒä¸€è‡´) ---
log_file = PROJECT_ROOT_DIR_FOR_APP / "rag_chat_gradio.log"  # æ—¥å¿—æ–‡ä»¶æ”¾åœ¨é¡¹ç›®æ ¹ç›®å½•
logging.basicConfig(
    level=getattr(config, 'LOG_LEVEL', logging.INFO),  # ä½¿ç”¨ config ä¸­çš„ LOG_LEVEL
    format=getattr(config, 'LOG_FORMAT', '%(asctime)s.%(msecs)03d - %(levelname)s - %(name)s - %(message)s'),
    datefmt=getattr(config, 'LOG_DATE_FORMAT', '%Y-%m-%d %H:%M:%S'),
    filename=log_file,
    filemode='a'  # é€šå¸¸ç”¨ 'a' (append) è€Œä¸æ˜¯ 'w' (overwrite)
)
console_handler = logging.StreamHandler(sys.stdout)  # è¾“å‡ºåˆ°æ§åˆ¶å°
console_handler.setFormatter(logging.Formatter(
    getattr(config, 'LOG_FORMAT', '%(asctime)s.%(msecs)03d - %(levelname)s - %(name)s - %(message)s'),
    datefmt=getattr(config, 'LOG_DATE_FORMAT', '%Y-%m-%d %H:%M:%S')
))
root_logger = logging.getLogger()
if not root_logger.hasHandlers() or all(
        isinstance(h, logging.FileHandler) for h in root_logger.handlers):  # é¿å…é‡å¤æ·»åŠ æ§åˆ¶å°handler
    root_logger.addHandler(console_handler)
root_logger.setLevel(getattr(config, 'LOG_LEVEL', logging.INFO))
logger = logging.getLogger(__name__)  # è·å–å½“å‰æ–‡ä»¶çš„ logger
# --------------------

# --- Global Variables (ä¸ä½ ç°æœ‰é€»è¾‘ä¿æŒä¸€è‡´) ---
kb: Optional[KnowledgeBase] = None
kb_initialized = False
generator_system_prompt_content: Optional[str] = None  # ä¿®æ”¹å˜é‡åä»¥æ¸…æ™°è¡¨ç¤ºæ˜¯å†…å®¹
prompts_loaded = False
# -----------------------

# --- Path handling (ä¸ä½ ç°æœ‰é€»è¾‘ä¿æŒä¸€è‡´ï¼Œç¡®ä¿è·¯å¾„æ­£ç¡®) ---
_prompt_dir = PROJECT_ROOT_DIR_FOR_APP / "prompts"
_gen_prompt_path = _prompt_dir / \
                   getattr(config, 'GENERATOR_SYSTEM_PROMPT_FILE', "generator_system_prompt.txt").split('/')[-1]
# (ä¸Šé¢å‡è®¾GENERATOR_SYSTEM_PROMPT_FILEå¯èƒ½åŒ…å«ç›¸å¯¹è·¯å¾„ï¼Œæˆ‘ä»¬åªå–æ–‡ä»¶åéƒ¨åˆ†ä¸_prompt_diræ‹¼æ¥)
# æ›´ç¨³å¥çš„æ–¹å¼æ˜¯ï¼Œå¦‚æœconfig.GENERATOR_SYSTEM_PROMPT_FILEæ˜¯ç»å¯¹è·¯å¾„å°±ç›´æ¥ç”¨ï¼Œå¦åˆ™ç›¸å¯¹PROJECT_ROOT_DIR
if not os.path.isabs(config.GENERATOR_SYSTEM_PROMPT_FILE):
    _gen_prompt_path = PROJECT_ROOT_DIR_FOR_APP / config.GENERATOR_SYSTEM_PROMPT_FILE
else:
    _gen_prompt_path = Path(config.GENERATOR_SYSTEM_PROMPT_FILE)


# --- Resource Loading Function (load_all_resources) ---
# è¿™ä¸ªå‡½æ•°ä¸ä½ ä¹‹å‰çš„ç‰ˆæœ¬åŸºæœ¬ä¸€è‡´ï¼Œåªæ˜¯KnowledgeBaseçš„åˆå§‹åŒ–å‚æ•°å˜äº†
def load_all_resources():
    global kb, generator_system_prompt_content, kb_initialized, prompts_loaded

    if kb_initialized and prompts_loaded:
        logger.debug("Resources already loaded.")
        return

    logger.info("Loading resources (Prompts, KB)...")
    start_time = time.time()

    # 1. Load Generator System Prompt
    if not prompts_loaded:
        try:
            logger.info(f"Attempting to load generator prompt from: {_gen_prompt_path}")
            if not _gen_prompt_path.is_file():
                raise FileNotFoundError(f"Generator prompt file not found: {_gen_prompt_path}")
            with open(_gen_prompt_path, "r", encoding="utf-8") as f:
                generator_system_prompt_content = f.read()  # èµ‹å€¼ç»™ä¿®æ”¹åçš„å˜é‡å
            if not generator_system_prompt_content:  # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
                logger.warning("Generator prompt file is empty. Using a default.")
                # generator_system_prompt_content = "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚" # å¯é€‰çš„é»˜è®¤å€¼
            prompts_loaded = True
            logger.info("Generator system prompt loaded successfully.")
        except Exception as e:
            logger.critical(f"Failed to load generator system prompt: {e}", exc_info=True)
            prompts_loaded = False

            # 2. Initialize Knowledge Base
    if not kb_initialized:
        try:
            logger.info("Initializing Knowledge Base...")
            # KnowledgeBase çš„ __init__ ç°åœ¨ä¸æ¥æ”¶ KNOWLEDGE_BASE_DIR å’Œ KNOWLEDGE_FILE_PATTERN
            # å®ƒå†…éƒ¨ä¼šä» config.py è¯»å– PROCESSED_DATA_DIR ç­‰è·¯å¾„æ¥åŠ è½½ç´¢å¼•
            kb = KnowledgeBase()
            kb_initialized = True
            logger.info("Knowledge Base initialized successfully.")
        except RuntimeError as e:  # KnowledgeBase åˆå§‹åŒ–å¤±è´¥æ—¶ä¼šæŠ›å‡º RuntimeError
            logger.critical(f"Knowledge Base initialization failed: {e}", exc_info=True)
            kb_initialized = False
        except Exception as e:
            logger.critical(f"Unexpected error during Knowledge Base initialization: {e}", exc_info=True)
            kb_initialized = False

    load_duration = time.time() - start_time
    if kb_initialized and prompts_loaded:  # ç¡®ä¿ä¸¤ä¸ªéƒ½æˆåŠŸ
        logger.info(f"Essential resources loaded in {load_duration:.2f} seconds.")
    else:
        # æ˜ç¡®æŒ‡å‡ºå“ªä¸ªå¤±è´¥äº†
        failed_resources = []
        if not kb_initialized: failed_resources.append("Knowledge Base")
        if not prompts_loaded: failed_resources.append("Generator Prompt")
        logger.error(
            f"Resource loading failed for: {', '.join(failed_resources)} (Duration: {load_duration:.2f}s). Check logs.")
        # åº”ç”¨æ˜¯å¦èƒ½ç»§ç»­è¿è¡Œå–å†³äºè¿™äº›èµ„æºçš„é‡è¦æ€§ã€‚KBé€šå¸¸æ˜¯æ ¸å¿ƒã€‚
        if not kb_initialized:
            raise RuntimeError(f"CRITICAL: KnowledgeBase failed to load. Application cannot continue.")


# --- Helper Functions (convert_gradio_to_openai, convert_openai_to_gradio_tuples, format_context_for_display) ---
# è¿™äº›ä¸ä½ ä¹‹å‰çš„ç‰ˆæœ¬å¯ä»¥ä¿æŒä¸€è‡´ï¼Œæˆ–è€…æ ¹æ®éœ€è¦å¾®è°ƒ format_context_for_display
# format_context_for_display ç°åœ¨åº”è¯¥èƒ½å¤„ç†åŒ…å« rerank_score, retrieved_from_paths ç­‰æ–°å…ƒæ•°æ®çš„å—

def format_context_for_display(chunks_data: List[Dict[str, Any]]) -> str:
    """Formats retrieved (and reranked) context chunks into a Markdown string for display."""
    if not chunks_data:
        return "ï¼ˆæœªå¼•ç”¨çŸ¥è¯†åº“ç‰‡æ®µæˆ–ä¸Šä¸‹æ–‡ä¸ºç©ºï¼‰"

    md_str = "### å‚è€ƒä¸Šä¸‹æ–‡ç‰‡æ®µ (å·²é‡æ’):\n\n"
    for i, chunk_data in enumerate(chunks_data):
        doc_name = chunk_data.get("doc_name", "æœªçŸ¥æ–‡æ¡£")
        page_num = chunk_data.get("page_number", "N/A")
        chunk_id = chunk_data.get("chunk_id", "N/A")
        text = chunk_data.get("text", "å†…å®¹ç¼ºå¤±")
        rerank_score = chunk_data.get("rerank_score")
        retrieved_from = list(chunk_data.get("retrieved_from_paths", {}).keys())  # è·å–å¬å›è·¯å¾„åˆ—è¡¨

        display_text = text[:250] + "..." if len(text) > 250 else text  # è°ƒæ•´é¢„è§ˆé•¿åº¦

        md_str += f"**ç‰‡æ®µ {i + 1}** (ID: `{chunk_id}`)\n"
        md_str += f"*æ¥æº*: `{doc_name}`, é¡µç : `{page_num}`\n"
        if rerank_score is not None:
            md_str += f"*Rerankå¾—åˆ†*: `{rerank_score:.4f}`\n"
        if retrieved_from:
            md_str += f"*å¬å›è·¯å¾„*: `{', '.join(retrieved_from)}`\n"
        md_str += f"> {display_text.replace(chr(10), ' ')}\n\n"  # æ›¿æ¢æ¢è¡Œä»¥ä¾¿UIæ˜¾ç¤º
    return md_str.strip()


# convert_gradio_to_openai å’Œ convert_openai_to_gradio_tuples ä¿æŒä¸å˜
# (ä»ä½ æä¾›çš„ app_gradio.py å¤åˆ¶è¿‡æ¥å³å¯)
def convert_gradio_to_openai(chat_history: Optional[List[Tuple[Optional[str], Optional[str]]]]) -> List[Dict[str, str]]:
    messages = []
    if not chat_history:
        return messages
    think_end_tag = "</think>"  # ä½ ä¹‹å‰çš„ä»£ç ä¸­æåˆ°äº†è¿™ä¸ªï¼Œè¿™é‡Œä¿ç•™
    for user_msg, assistant_msg in chat_history:
        if user_msg is not None and user_msg.strip():
            messages.append({"role": "user", "content": user_msg.strip()})
        if assistant_msg is not None and assistant_msg.strip():
            cleaned_assistant_msg = assistant_msg
            # ä½ çš„ä»£ç ä¸­æœ‰ä¸€ä¸ªç§»é™¤ <think> æ ‡ç­¾çš„é€»è¾‘ï¼Œæˆ‘ä»¬ä¿ç•™å®ƒï¼Œ
            # å°½ç®¡æ–°çš„æµç¨‹ä¸­ CoT å¯èƒ½é€šè¿‡ç‹¬ç«‹çš„UIç»„ä»¶å±•ç¤º
            first_think_end_index = assistant_msg.find(think_end_tag)
            if first_think_end_index != -1:
                cleaned_assistant_msg = assistant_msg[first_think_end_index + len(think_end_tag):].strip()
            if cleaned_assistant_msg:  # åªæ·»åŠ æ¸…ç†åéç©ºçš„å†…å®¹
                messages.append({"role": "assistant", "content": cleaned_assistant_msg})
    return messages


def convert_openai_to_gradio_tuples(messages_history: List[Dict[str, str]]) -> List[
    Tuple[Optional[str], Optional[str]]]:
    gradio_history = []
    user_msg_buffer = None
    for msg in messages_history:
        role = msg.get("role")
        content = msg.get("content")
        if role == "user":
            if user_msg_buffer is not None:  # å‰ä¸€ä¸ªuseræ¶ˆæ¯æ²¡æœ‰é…å¯¹çš„assistantæ¶ˆæ¯
                gradio_history.append((user_msg_buffer, None))
            user_msg_buffer = content
        elif role == "assistant":
            gradio_history.append((user_msg_buffer, content))
            user_msg_buffer = None
    if user_msg_buffer is not None:  # å¤„ç†æœ€åä¸€ä¸ªæ˜¯ç”¨æˆ·æ¶ˆæ¯çš„æƒ…å†µ
        gradio_history.append((user_msg_buffer, None))
    return gradio_history


async def respond(
        message: str,
        chat_history: Optional[List[Tuple[Optional[str], Optional[str]]]],
        # å‡è®¾ä½ çš„ Gradio layout outputs é¡ºåºæ˜¯: chatbot, context_display, rewritten_query_display
        # å¦‚æœä½ æ·»åŠ äº† thinking_process_display, é‚£ä¹ˆ yield çš„åˆ—è¡¨ä¹Ÿéœ€è¦å¯¹åº”å¢åŠ 
) -> AsyncGenerator[
    List[Union[List[Tuple[Optional[str], Optional[str]]], str, gr.Markdown, gr.Textbox, gr.update]], None]:
    global kb, kb_initialized, generator_system_prompt_content  # ä½¿ç”¨å…¨å±€åŠ è½½çš„èµ„æº

    # --- 1. åˆå§‹åŒ–å’Œè¾“å…¥å¤„ç† ---
    chat_history_tuples = chat_history or []
    # å°†Gradioçš„èŠå¤©å†å²è½¬æ¢ä¸ºOpenAIæ ¼å¼ï¼Œç”¨äºä¼ é€’ç»™RAG pipeline
    messages_history_openai = convert_gradio_to_openai(chat_history_tuples)

    # ç”¨äºUIæ›´æ–°çš„å˜é‡
    # æ³¨æ„ï¼šGradio çš„ chatbot ç»„ä»¶æœŸæœ›çš„æ˜¯ä¸€ä¸ª [(user_msg, assistant_msg), ...] æ ¼å¼çš„å®Œæ•´åˆ—è¡¨
    # æˆ‘ä»¬éœ€è¦ç»´æŠ¤ä¸€ä¸ªå†…éƒ¨çš„ OpenAI æ ¼å¼å†å²ï¼Œå¹¶åœ¨æ¯æ¬¡ yield æ—¶è½¬æ¢ä¸º Gradio æ ¼å¼
    current_openai_history_for_display = list(messages_history_openai)  # å¤åˆ¶ä¸€ä»½ç”¨äºæ˜¾ç¤º
    if message and message.strip():
        current_openai_history_for_display.append({"role": "user", "content": message.strip()})

    assistant_response_accumulator = ""  # ç”¨äºç´¯ç§¯ content_delta
    thinking_process_accumulator = ""  # ç”¨äºç´¯ç§¯ reasoning_delta

    # åˆå§‹åŒ–UIç»„ä»¶çš„æ˜¾ç¤ºå†…å®¹
    chatbot_display_list = convert_openai_to_gradio_tuples(current_openai_history_for_display + [{"role": "assistant", "content": "ğŸ¤” å¤„ç†ä¸­..."}])
    context_md_str = "(ä¸Šä¸‹æ–‡ä¿¡æ¯å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...)"
    rewritten_query_str = "(é‡å†™åçš„æŸ¥è¯¢å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...)"
    # thinking_md_str = "(æ¨¡å‹æ€è€ƒè¿‡ç¨‹å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...)" # å¦‚æœæœ‰è¿™ä¸ªUIç»„ä»¶

    # è¾…åŠ©å‡½æ•°ï¼Œç”¨äºæ„é€ å¹¶ yield Gradio æ›´æ–°
    # å‡è®¾ä½ çš„Gradioè¾“å‡ºé¡ºåºæ˜¯: chatbot, context_display, rewritten_query_display, [å¯é€‰: thinking_display]
    # ä½ éœ€è¦æ ¹æ®ä½ çš„ web_ui.layout.py ä¸­ create_layout å‡½æ•°å®šä¹‰çš„ outputs é¡ºåºæ¥è°ƒæ•´ yield çš„åˆ—è¡¨
    async def yield_gradio_update(
            current_assistant_reply: str = "ğŸ¤” å¤„ç†ä¸­...",
            context_md: Optional[str] = None,
            rewritten_query: Optional[str] = None,
            thinking_md: Optional[str] = None  # å¦‚æœæœ‰æ€è€ƒè¿‡ç¨‹æ˜¾ç¤ºç»„ä»¶
    ):
        # å‡†å¤‡ chatbot çš„æ˜¾ç¤ºåˆ—è¡¨
        history_for_chatbot = current_openai_history_for_display + [
            {"role": "assistant", "content": current_assistant_reply}]
        chatbot_tuples = convert_openai_to_gradio_tuples(history_for_chatbot)

        outputs_to_yield = [chatbot_tuples]
        outputs_to_yield.append(gr.Markdown(value=context_md) if context_md is not None else gr.Markdown(update=True))
        outputs_to_yield.append(
            gr.Textbox(value=rewritten_query) if rewritten_query is not None else gr.Textbox(update=True))

        # å¦‚æœä½ çš„UIä¸­æœ‰ç¬¬å››ä¸ªè¾“å‡ºç»„ä»¶ç”¨äºæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ (ä¾‹å¦‚ä¸€ä¸ªåä¸º thinking_process_display çš„ gr.Markdown)
        # if thinking_md is not None:
        #     outputs_to_yield.append(gr.Markdown(value=thinking_md, visible=True))
        # else:
        #     outputs_to_yield.append(gr.Markdown(update=True)) # æˆ–è€… gr.Markdown(visible=False)

        yield outputs_to_yield

    # --- æ£€æŸ¥KBæ˜¯å¦åˆå§‹åŒ– ---
    if not kb or not kb_initialized:
        error_msg = "é”™è¯¯ï¼šçŸ¥è¯†åº“æœªåˆå§‹åŒ–æˆ–åŠ è½½å¤±è´¥ï¼Œæ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ã€‚"
        logger.error(error_msg)
        yield_gradio_update(current_assistant_reply=error_msg, context_md="é”™è¯¯", rewritten_query="é”™è¯¯")
        return

    # --- å¯¹ç©ºæ¶ˆæ¯çš„å¿«é€Ÿå¤„ç† ---
    if not message or not message.strip():
        yield_gradio_update(current_assistant_reply="(è¯·è¾“å…¥æ‚¨çš„é—®é¢˜)", context_md="æ— ä¸Šä¸‹æ–‡",
                                  rewritten_query="æ— æŸ¥è¯¢")
        return

    logger.info(f"Respond function called with message: '{message[:100]}...'")

    # --- åˆå§‹UIæ›´æ–°ï¼šæ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯å’Œâ€œå¤„ç†ä¸­â€ ---
    yield_gradio_update()  # ä½¿ç”¨é»˜è®¤çš„ "ğŸ¤” å¤„ç†ä¸­..."

    # --- 2. è°ƒç”¨æ ¸å¿ƒ RAG æµç¨‹ ---
    final_rewritten_query = message.strip()  # é»˜è®¤å€¼
    final_context_md = "(æœªèƒ½æ£€ç´¢åˆ°ç›¸å…³ä¸Šä¸‹æ–‡)"

    try:
        async for event in execute_rag_flow(
                user_query=message.strip(),
                chat_history_openai=messages_history_openai,  # ä¼ é€’è½¬æ¢åçš„å†å²
                kb_instance=kb
        ):
            event_type = event.get("type")

            if event_type == "status":
                stage = event.get("stage", "unknown_stage")
                status_message = event.get("message", "å¤„ç†ä¸­...")
                logger.info(f"[UI_UPDATE] Status: [{stage}] {status_message}")
                yield_gradio_update(current_assistant_reply=f"â³ {status_message}",
                                          context_md=final_context_md,  # ä¿æŒä¹‹å‰çš„ä¸Šä¸‹æ–‡æˆ–åˆå§‹å€¼
                                          rewritten_query=final_rewritten_query)

            elif event_type == "rewritten_query_result":
                final_rewritten_query = event.get("rewritten_text", final_rewritten_query)
                yield_gradio_update(current_assistant_reply="â³ æ£€ç´¢ä¸­...",
                                          context_md=final_context_md,
                                          rewritten_query=final_rewritten_query)

            elif event_type == "reranked_context_for_display":
                reranked_chunks_for_ui = event.get("chunks", [])
                final_context_md = format_context_for_display(reranked_chunks_for_ui)  # ä½¿ç”¨ä½ å·²æœ‰çš„æ ¼å¼åŒ–å‡½æ•°
                yield_gradio_update(current_assistant_reply="â³ ç”Ÿæˆç­”æ¡ˆä¸­...",
                                          context_md=final_context_md,
                                          rewritten_query=final_rewritten_query)

            elif event_type == "reasoning_delta":
                thinking_token = event.get("text", "")
                thinking_process_accumulator += thinking_token
                # --- æ›´æ–°æ€è€ƒè¿‡ç¨‹UIç»„ä»¶ ---
                # å¦‚æœä½ æœ‰ä¸€ä¸ªä¸“é—¨çš„ thinking_process_display: gr.Markdown ç»„ä»¶
                yield [
                    gr.Chatbot(update=True), # chatbotä¸»å›å¤åŒºå¯ä»¥æ˜¾ç¤º"æ­£åœ¨æ€è€ƒ..."
                    gr.Markdown(update=True), # context_display
                    gr.Textbox(update=True),  # rewritten_query_display
                    # gr.Markdown(value=format_thinking_for_display(thinking_process_accumulator)) # å‡è®¾ä½ æœ‰è¿™ä¸ªæ ¼å¼åŒ–å‡½æ•°
                ]
                # ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬æš‚æ—¶ä¸åœ¨ä¸»èŠå¤©æµä¸­æ··åˆæ€è€ƒè¿‡ç¨‹ï¼Œä½†ä½ å¯ä»¥è®°å½•å®ƒ
                logger.debug(f"Reasoning delta: {thinking_token}")


            elif event_type == "content_delta":
                content_token = event.get("text", "")
                print(content_token,end="")
                assistant_response_accumulator += content_token
                yield_gradio_update(current_assistant_reply=assistant_response_accumulator + "â–Œ",  # æ‰“å­—æœºæ•ˆæœ
                                          context_md=final_context_md,
                                          rewritten_query=final_rewritten_query)

            elif event_type == "final_answer_complete":
                assistant_response_accumulator = event.get("full_text", assistant_response_accumulator).strip()
                # full_reasoning = event.get("full_reasoning", thinking_process_accumulator).strip()
                # final_context_chunk_ids = event.get("final_context_chunk_ids", [])
                logger.info(f"Final answer generated (length: {len(assistant_response_accumulator)}).")
                # æœ€åçš„UIæ›´æ–°ä¼šåœ¨pipeline_endæ—¶è¿›è¡Œï¼Œä»¥ç¡®ä¿æ˜¯æœ€ç»ˆçŠ¶æ€

            elif event_type == "error":
                error_stage = event.get("stage", "unknown")
                error_message = event.get("message", "æœªçŸ¥é”™è¯¯")
                logger.error(f"Pipeline error at stage '{error_stage}': {error_message}")
                assistant_response_accumulator = f"âŒ åœ¨å¤„ç†é˜¶æ®µ '{error_stage}' å‘ç”Ÿé”™è¯¯: {error_message}"
                # yield æœ€ç»ˆé”™è¯¯çŠ¶æ€ç„¶åä¸­æ–­
                yield_gradio_update(current_assistant_reply=assistant_response_accumulator,
                                          context_md=final_context_md,
                                          rewritten_query=final_rewritten_query)
                return  # æå‰ç»“æŸ respond å‡½æ•°

            elif event_type == "pipeline_end":
                logger.info(f"Pipeline ended with reason: {event.get('reason')}")
                # å¦‚æœæ˜¯å› ä¸ºé”™è¯¯ç»“æŸï¼Œassistant_response_accumulatorå¯èƒ½å·²ç»æ˜¯é”™è¯¯ä¿¡æ¯
                # å¦‚æœæ˜¯æ­£å¸¸ç»“æŸï¼Œå®ƒåº”è¯¥æ˜¯ç´¯ç§¯çš„ç­”æ¡ˆ
                if not assistant_response_accumulator and event.get('reason') == "no_context_found_after_retrieval":
                    assistant_response_accumulator = "æŠ±æ­‰ï¼Œæœªèƒ½æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›´æ¥ç›¸å…³çš„çŸ¥è¯†ã€‚"
                elif not assistant_response_accumulator and event.get('reason') == "no_context_found_after_reranking":
                    assistant_response_accumulator = "æŠ±æ­‰ï¼Œä¿¡æ¯ç­›é€‰åæœªèƒ½æ‰¾åˆ°è¶³å¤Ÿç›¸å…³çš„å†…å®¹ã€‚"

                yield_gradio_update(current_assistant_reply=assistant_response_accumulator.strip(),  # ç§»é™¤å¯èƒ½çš„å…‰æ ‡
                                          context_md=final_context_md,
                                          rewritten_query=final_rewritten_query)
                return  # æµç¨‹æ­£å¸¸æˆ–å¤„ç†å®Œæ¯•åœ°ç»“æŸ

    except Exception as e:
        logger.error(f"Error in respond's main RAG flow execution: {e}", exc_info=True)
        error_text = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‘ç”Ÿç³»ç»Ÿå†…éƒ¨é”™è¯¯ã€‚"
        # ç¡®ä¿ chat_history_tuples å’Œå…¶ä»–UIå…ƒç´ æ˜¯æœ€æ–°çš„çŠ¶æ€
        # current_openai_history_for_display åŒ…å«äº†å½“å‰çš„ç”¨æˆ·è¾“å…¥
        # æˆ‘ä»¬éœ€è¦å°†é”™è¯¯ä¿¡æ¯ä½œä¸ºåŠ©æ‰‹çš„æœ€åå›å¤
        error_chatbot_history = current_openai_history_for_display + [{"role": "assistant", "content": error_text}]
        yield [
            convert_openai_to_gradio_tuples(error_chatbot_history),
            final_context_md if final_context_md else "å‘ç”Ÿé”™è¯¯ï¼Œæ— ä¸Šä¸‹æ–‡ã€‚",
            final_rewritten_query if final_rewritten_query else "(æŸ¥è¯¢å¤„ç†å‡ºé”™)"
        ]
    finally:
        logger.info(f"Respond function finished for query: '{message[:50]}...'")

# --- Clear History Action Function ---
# This function is passed to layout.py and called by the clear button click event
def clear_history_action():
    """Action to clear chat history and related displays."""
    logger.info("Clearing chat history action triggered.")
    # Returns default values for the components listed in clear_button.click outputs
    # chatbot, msg_input, context_display, rewritten_query_display
    return (
        [],                               # chatbot (empty list for tuple format)
        "",                               # msg_input (clear input box)
        "ä¸Šä¸‹æ–‡å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...",            # context_display
        "é‡æ„åçš„æŸ¥è¯¢å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ..."      # rewritten_query_display
    )


# --- Main Application Entry Point ---
if __name__ == "__main__":
    # ç¡®ä¿æ ¸å¿ƒä¾èµ–å·²å®‰è£… (ä½ ä¹‹å‰çš„ä»£ç ä¸­å·²ç»æœ‰è¿™ä¸ªæ£€æŸ¥)
    try:
        import gradio  # type: ignore
        # import aiohttp # aiohttp å·²ç»åœ¨ respond å‡½æ•°ä¸­è¢«å¯¼å…¥å’Œä½¿ç”¨
    except ImportError as e:
        missing_lib = str(e).split("'")[-2]  # å°è¯•æå–ç¼ºå¤±çš„åº“å
        print(f"é”™è¯¯: æ ¸å¿ƒä¾èµ– {missing_lib} æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install {missing_lib.lower()}")
        exit(1)

    logger.info("Starting RAG Gradio Application...")

    # 1. æ‰§è¡Œåˆå§‹èµ„æºåŠ è½½
    # è¿™ä¸ªå‡½æ•°ä¼šè®¾ç½®å…¨å±€çš„ kb_instance å’Œ generator_system_prompt_content
    try:
        load_all_resources()
        # åœ¨ load_all_resources å†…éƒ¨ï¼Œæˆ‘ä»¬å·²ç»æ·»åŠ äº†å¯¹ kb_initialized å’Œ prompts_loaded çš„æ£€æŸ¥
        # å¦‚æœå…³é”®èµ„æºåŠ è½½å¤±è´¥ï¼Œå®ƒä¼šæŠ›å‡º RuntimeError æˆ–æ‰“å°é”™è¯¯å¹¶å¯èƒ½å¯¼è‡´åº”ç”¨æ— æ³•æ­£å¸¸å·¥ä½œ
        if not kb_initialized:  # åŒé‡æ£€æŸ¥
            logger.critical("KnowledgeBase æœªèƒ½åœ¨ load_all_resources ä¸­æˆåŠŸåˆå§‹åŒ–ã€‚åº”ç”¨å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œã€‚")
            # ä½ å¯ä»¥é€‰æ‹©åœ¨è¿™é‡Œå¼ºåˆ¶é€€å‡ºï¼Œæˆ–è€…è®© Gradio UI æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
            # exit(1)
        if not prompts_loaded:  # åŒé‡æ£€æŸ¥
            logger.warning("Generator system prompt æœªèƒ½åœ¨ load_all_resources ä¸­æˆåŠŸåŠ è½½ã€‚å¯èƒ½ä¼šä½¿ç”¨é»˜è®¤æç¤ºã€‚")

    except RuntimeError as e:  # æ•è· load_all_resources ä¸­å¯èƒ½æŠ›å‡ºçš„å…³é”®é”™è¯¯
        logger.critical(f"åº”ç”¨å¯åŠ¨å¤±è´¥ï¼Œå¿…è¦çš„èµ„æºåŠ è½½æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        print(f"\nCRITICAL ERROR: Application failed to start due to resource loading issue: {e}\n")
        exit(1)
    except Exception as initial_load_err:
        logger.critical(f"åº”ç”¨å¯åŠ¨æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {initial_load_err}", exc_info=True)
        print(f"\nCRITICAL ERROR: Unexpected error during application startup: {initial_load_err}\n")
        exit(1)

    # 2. åˆ›å»º Gradio UI å¸ƒå±€
    # create_layout å‡½æ•°ä¼šä» web_ui.layout å¯¼å…¥
    # å®ƒæ¥æ”¶ respond å‡½æ•°ã€clear_history_action å‡½æ•°å’Œ load_all_resources å‡½æ•°ä½œä¸ºå‚æ•°
    # å¹¶è¿”å›ä¸€ä¸ª gr.Blocks å®ä¾‹ (demo_instance)
    try:
        # ç¡®ä¿ config å¯¹è±¡å’Œæ‰€æœ‰éœ€è¦çš„å¸¸é‡éƒ½èƒ½è¢« create_layout æˆ–å…¶å†…éƒ¨é€»è¾‘è®¿é—®
        # (å¦‚æœ create_layout ç›´æ¥ä» config æ¨¡å—è¯»å–é…ç½®çš„è¯)
        demo_instance = create_layout(
            respond_fn=respond,  # æˆ‘ä»¬é‡æ„åçš„å¼‚æ­¥ respond å‡½æ•°
            clear_fn=clear_history_action,  # ä½ å·²æœ‰çš„æ¸…ç©ºå†å²å‡½æ•°
            load_fn=load_all_resources,  # èµ„æºåŠ è½½å‡½æ•°ï¼Œç”¨äº "é‡æ–°åŠ è½½èµ„æº" æŒ‰é’®
            # æ ¹æ®ä½ çš„ create_layout å‡½æ•°å®šä¹‰ï¼Œå¯èƒ½è¿˜éœ€è¦å…¶ä»–å‚æ•°
        )
        logger.info("Gradio UI layout created successfully.")
    except Exception as layout_err:
        logger.critical(f"åˆ›å»º Gradio UI å¸ƒå±€æ—¶å¤±è´¥: {layout_err}", exc_info=True)
        print(f"\nCRITICAL ERROR: Failed to create Gradio UI layout: {layout_err}\n")
        exit(1)

    # 3. å¯åŠ¨ Gradio åº”ç”¨
    # ä» config.py ä¸­è¯»å– Gradio å¯åŠ¨å‚æ•°
    server_name = getattr(config, "GRADIO_SERVER_NAME", "0.0.0.0")
    server_port = getattr(config, "GRADIO_SERVER_PORT", 8848)  # ä½ ä¹‹å‰ç”¨çš„æ˜¯8848
    share_gradio = getattr(config, "GRADIO_SHARE", False)
    gradio_user = getattr(config, "GRADIO_USER", None)
    gradio_password = getattr(config, "GRADIO_PASSWORD", None)
    concurrency_limit = getattr(config, "GRADIO_CONCURRENCY_LIMIT", 8)  # ä¸ä½ ä¹‹å‰ä»£ç ä¸€è‡´

    auth_credentials = None
    if gradio_user and gradio_password:
        auth_credentials = (gradio_user, gradio_password)
        logger.info(f"Gradioè®¤è¯å·²å¯ç”¨ï¼Œç”¨æˆ·: {gradio_user}")
    else:
        logger.info("Gradioè®¤è¯æœªå¯ç”¨ã€‚")

    logger.info(f"å‡†å¤‡åœ¨ {server_name}:{server_port} å¯åŠ¨ Gradio åº”ç”¨...")
    print(f"Gradio åº”ç”¨æ­£åœ¨å¯åŠ¨ï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—® http://{server_name}:{server_port}")
    if server_name == "0.0.0.0":
        print(f"å¦‚æœåœ¨æœ¬æœºè¿è¡Œï¼Œä¹Ÿå¯ä»¥é€šè¿‡ http://127.0.0.1:{server_port} è®¿é—®")

    try:
        demo_instance.queue(default_concurrency_limit=concurrency_limit).launch(
            server_name=server_name,
            server_port=server_port,
            share=share_gradio,
            auth=auth_credentials,
            # prevent_thread_lock=True, # å¯¹äºå¼‚æ­¥å‡½æ•°ï¼Œæœ‰æ—¶éœ€è¦è¿™ä¸ª
            # allowed_paths=[str(PROJECT_ROOT_DIR_FOR_APP / "your_static_folder")] # å¦‚æœéœ€è¦æä¾›é™æ€æ–‡ä»¶
        )
        # launch() æ˜¯ä¸€ä¸ªé˜»å¡è°ƒç”¨ï¼Œç¨‹åºä¼šåœ¨è¿™é‡Œç­‰å¾…ç›´åˆ° Gradio æœåŠ¡åœæ­¢
        logger.info(f"Gradio app has been shut down.")

    except Exception as launch_err:
        logger.critical(f"å¯åŠ¨ Gradio åº”ç”¨æ—¶å¤±è´¥: {launch_err}", exc_info=True)
        print(f"\nCRITICAL ERROR: Failed to launch Gradio app: {launch_err}\n")
        exit(1)
